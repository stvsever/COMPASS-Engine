{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "05f4b962",
      "metadata": {},
      "source": [
        "# COMPASS on HPC: Operational Guide\n",
        "\n",
        "**Audience:** researchers and engineers running COMPASS on Slurm-managed GPU clusters.\n",
        "\n",
        "This notebook is a didactic operations guide. It explains the infrastructure components, the execution workflow, and the practical monitoring/debugging steps required to run COMPASS reliably on HPC.\n",
        "\n",
        "> Execution should happen via terminal + Slurm jobs. The command cells below are examples you can copy into your shell.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bad2fa83",
      "metadata": {},
      "source": [
        "## Table of Contents\n",
        "\n",
        "1. Why HPC for COMPASS\n",
        "2. HPC Infrastructure Components\n",
        "3. Storage and Data Layout\n",
        "4. End-to-End Workflow (00 -> 05)\n",
        "5. Command Walkthrough\n",
        "6. Slurm Directives Explained\n",
        "7. Monitoring and Log Interpretation\n",
        "8. Troubleshooting Playbook\n",
        "9. Performance and Stability Tuning\n",
        "10. Reproducibility and Validation\n",
        "11. Adaptation Checklist for Other Clusters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c341e1c0",
      "metadata": {},
      "source": [
        "## 1) Why HPC for COMPASS\n",
        "\n",
        "COMPASS can run with either public APIs or local models. HPC local execution is useful when you need:\n",
        "\n",
        "- strict data locality / privacy constraints,\n",
        "- reproducible runtime control,\n",
        "- budget control for repeated clinical validation runs,\n",
        "- deterministic infrastructure for benchmarking.\n",
        "\n",
        "Tradeoff: local single-GPU inference is usually slower than hosted APIs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbae2d3d",
      "metadata": {},
      "source": [
        "## 2) HPC Infrastructure Components\n",
        "\n",
        "### Login node\n",
        "- Used for SSH access, editing files, and submitting jobs.\n",
        "- Often has restrictions on heavy compute.\n",
        "\n",
        "### Compute node\n",
        "- Runs the workload after Slurm allocation.\n",
        "- Provides GPUs/CPUs/RAM according to your `#SBATCH` request.\n",
        "\n",
        "### Slurm scheduler\n",
        "- Queues, allocates, and tracks jobs.\n",
        "- Common commands: `sbatch`, `squeue`, `sacct`, `scancel`.\n",
        "\n",
        "### Apptainer container runtime\n",
        "- Runs a reproducible environment on compute nodes.\n",
        "- Uses `--nv` to expose NVIDIA drivers inside the container.\n",
        "- Uses `--bind` to mount your project/models/data into the container.\n",
        "\n",
        "### Python environment\n",
        "- A virtual environment provides Python dependencies for COMPASS.\n",
        "- You typically activate it inside the container execution block.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e11ef75f",
      "metadata": {},
      "source": [
        "## 3) Storage and Data Layout\n",
        "\n",
        "Typical recommended layout on shared storage:\n",
        "\n",
        "- Project: `~/compass_pipeline/multi_agent_system`\n",
        "- Container image: `~/compass_containers/<image>.sif`\n",
        "- Virtualenv: `~/compass_venv/`\n",
        "- Models: `~/compass_models/`\n",
        "- Logs: `~/compass_pipeline/multi_agent_system/logs/`\n",
        "- Participant data: `../data/__FEATURES__/HPC_data/participant_ID<id>/`\n",
        "\n",
        "Notes:\n",
        "- Many clusters use shared parallel filesystems; large model and data directories should live on those.\n",
        "- Keep secrets in environment variables or `.env` locally; do not commit them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "399ea5ef",
      "metadata": {},
      "source": [
        "## 4) End-to-End Workflow (`hpc/`)\n",
        "\n",
        "| Step | Script | Goal |\n",
        "|---|---|---|\n",
        "| 00 | `00_deploy_and_run.sh` | Optional: copy repo/data to HPC and SSH in |\n",
        "| 01 | `01_check_status.sh` | Validate prerequisites before spending GPU hours |\n",
        "| 02 | `02_setup_environment.sh` | Build container + virtualenv runtime |\n",
        "| 03 | `03_download_models.sh` | Fetch and patch model artifacts |\n",
        "| 04 | `04_submit_single.sh` | Smoke-test one participant end-to-end |\n",
        "| 05 | `05_submit_batch.sh` | Run sequential cohort validation on single GPU |\n",
        "\n",
        "**Recommended policy:** never run step `05` before step `04` succeeds cleanly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e454063",
      "metadata": {},
      "source": [
        "## 5) Command Walkthrough\n",
        "\n",
        "Set project root first:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "991d321c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Terminal commands (copy/paste in shell)\n",
        "cd ~/compass_pipeline/multi_agent_system\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90af28f0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 00 (optional): deploy from your workstation\n",
        "bash hpc/00_deploy_and_run.sh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3413cc9e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 01: pre-flight checks\n",
        "bash hpc/01_check_status.sh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c600f3d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 02: environment setup (container + venv)\n",
        "bash hpc/02_setup_environment.sh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "348d96c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 03: download models\n",
        "bash hpc/03_download_models.sh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bfeeca9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 04: single participant smoke test\n",
        "bash hpc/04_submit_single.sh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a58ab41",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 05: sequential batch execution\n",
        "bash hpc/05_submit_batch.sh\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "779dea1c",
      "metadata": {},
      "source": [
        "## 6) Slurm Directives Explained\n",
        "\n",
        "Submission scripts use `#SBATCH` headers. Typical single-GPU fields:\n",
        "\n",
        "- `--partition`: queue/partition name for your cluster.\n",
        "- `--gres=gpu:<type>:1`: request one GPU of a specific type.\n",
        "- `--cpus-per-task`: CPU workers for preprocessing/runtime overhead.\n",
        "- `--mem`: host RAM budget (separate from GPU VRAM).\n",
        "- `--time`: wall-clock cap; job is terminated after this limit.\n",
        "- `--output` / `--error`: stdout/stderr log destinations.\n",
        "\n",
        "If your cluster names resources differently, adapt these values first.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa5bc798",
      "metadata": {},
      "source": [
        "## 7) Monitoring and Log Interpretation\n",
        "\n",
        "### Runtime monitoring\n",
        "```bash\n",
        "squeue -u $USER\n",
        "```\n",
        "\n",
        "### Live logs\n",
        "```bash\n",
        "tail -f logs/compass_single_<JOBID>.out\n",
        "tail -f logs/compass_batch_<JOBID>.out\n",
        "```\n",
        "\n",
        "### Error logs\n",
        "```bash\n",
        "cat logs/compass_single_<JOBID>.err\n",
        "cat logs/compass_batch_<JOBID>.err\n",
        "```\n",
        "\n",
        "Notes:\n",
        "- `.out` captures runtime milestones and tool/agent progress.\n",
        "- `.err` captures exceptions/tracebacks and scheduler/runtime failures.\n",
        "- Inspect logs with `cat`, `less`, or `tail`; do not execute them as scripts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc28fca2",
      "metadata": {},
      "source": [
        "## 8) Troubleshooting Playbook\n",
        "\n",
        "### Job exits immediately\n",
        "- Check path assumptions (`PROJECT_DIR`, `MODELS_DIR`, `VENV_DIR`, container path).\n",
        "- Run `01_check_status.sh` again.\n",
        "\n",
        "### `apptainer` command missing\n",
        "- Verify cluster module/environment policy for compute nodes.\n",
        "- Confirm script is actually running on a compute node, not a login node.\n",
        "\n",
        "### Model initialization failure\n",
        "- Confirm model directories are complete and readable.\n",
        "- Check quantization/runtime flags in logs.\n",
        "- Reduce context window if memory pressure appears.\n",
        "\n",
        "### JSON/structured-output instability in local model\n",
        "- Lower generation temperature.\n",
        "- Tighten output token limits for tool calls.\n",
        "- Keep retry/repair strategy enabled in orchestrated flow.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "143b62cf",
      "metadata": {},
      "source": [
        "## 9) Performance and Stability Tuning\n",
        "\n",
        "Primary levers:\n",
        "\n",
        "- context size (`--max_tokens`, local max model len),\n",
        "- per-role token budgets (`--max_agent_input`, `--max_agent_output`, `--max_tool_input`, `--max_tool_output`),\n",
        "- runtime backend flags (local engine, quantization, eager mode, memory utilization),\n",
        "- chunking and evidence extraction strategy.\n",
        "\n",
        "Guideline:\n",
        "- First optimize for **successful deterministic completion**.\n",
        "- Then optimize latency while keeping output quality acceptable.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5b0d210",
      "metadata": {},
      "source": [
        "## 10) Reproducibility and Validation\n",
        "\n",
        "Best practices:\n",
        "\n",
        "- Keep scripts and config under version control.\n",
        "- Capture exact script/runtime info in `.out` logs (scripts print fingerprints and runtime settings).\n",
        "- Keep per-participant artifacts in `results/` for auditability.\n",
        "- Validate with fixed cohorts and compare confusion-matrix metrics across runs.\n",
        "- Change one parameter family at a time when benchmarking.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae19da6c",
      "metadata": {},
      "source": [
        "## 11) Adaptation Checklist for Other Clusters\n",
        "\n",
        "Before first run on a new HPC:\n",
        "\n",
        "1. Update Slurm directives (`partition`, GPU resource syntax, memory/time limits).\n",
        "2. Update path variables in `hpc/04_submit_single.sh` and `hpc/05_submit_batch.sh`.\n",
        "3. Confirm data root and participant folder conventions.\n",
        "4. Validate `01 -> 04` end-to-end before any `05` batch run.\n",
        "\n",
        "Related docs:\n",
        "- `hpc/README.md`\n",
        "- `utils/batch_run.py`\n",
        "- `main.py`\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "python",
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "version": "3.10",
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
