{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# COMPASS on HPC: Operational Guide\n",
        "\n",
        "**Audience:** researchers and engineers running COMPASS on Slurm-managed GPU clusters.\n",
        "\n",
        "This notebook is a didactic operations guide. It explains the architecture, workflow, and decision points required to run COMPASS reliably on HPC infrastructure.\n",
        "\n",
        "> Execution should happen via terminal + Slurm jobs. The command cells below are examples you can copy into your shell.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of Contents\n",
        "\n",
        "1. Why HPC for COMPASS\n",
        "2. Core HPC Components\n",
        "3. End-to-End Workflow (`00` -> `04`)\n",
        "4. Command Walkthrough\n",
        "5. Slurm Directives Explained\n",
        "6. Monitoring and Log Interpretation\n",
        "7. Troubleshooting Playbook\n",
        "8. Performance and Stability Tuning\n",
        "9. Reproducibility and Validation\n",
        "10. Adaptation Checklist for Other Clusters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Why HPC for COMPASS\n",
        "\n",
        "COMPASS can run with either public APIs or local models. HPC local execution is useful when you need:\n",
        "\n",
        "- strict data locality / privacy constraints,\n",
        "- reproducible runtime control,\n",
        "- budget control for repeated clinical validation runs,\n",
        "- deterministic infrastructure for benchmarking.\n",
        "\n",
        "Tradeoff: local single-GPU inference is usually slower than hosted APIs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Core HPC Components\n",
        "\n",
        "### Login Node\n",
        "- Used for SSH access, editing files, and submitting jobs.\n",
        "- Not intended for heavy GPU compute.\n",
        "\n",
        "### Compute Node\n",
        "- Runs the actual workload after Slurm allocation.\n",
        "- Provides GPUs/CPUs/RAM according to your `#SBATCH` request.\n",
        "\n",
        "### Slurm\n",
        "- Scheduler that queues, allocates, and tracks jobs.\n",
        "- Main commands: `sbatch`, `squeue`, `sacct`, `scancel`.\n",
        "\n",
        "### Apptainer\n",
        "- Container runtime for reproducible environments on HPC.\n",
        "- Keeps runtime dependency stack stable across nodes.\n",
        "\n",
        "### Python Virtual Environment\n",
        "- Project-level dependency layer used inside the container runtime.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) End-to-End Workflow (`hpc`)\n",
        "\n",
        "| Step | Script | Goal |\n",
        "|---|---|---|\n",
        "| 00 | `00_check_status.sh` | Validate prerequisites before spending GPU hours |\n",
        "| 01 | `01_setup_environment.sh` | Build container + virtualenv runtime |\n",
        "| 02 | `02_download_models.sh` | Fetch and patch model artifacts |\n",
        "| 03 | `03_submit_single.sh` | Smoke-test one participant |\n",
        "| 04 | `04_submit_batch.sh` | Run sequential cohort validation on single GPU |\n",
        "\n",
        "**Recommended policy:** never run step `04` before step `03` succeeds cleanly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Command Walkthrough\n",
        "\n",
        "Set project root first:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Terminal commands (copy/paste in shell)\n",
        "cd ~/compass_pipeline/multi_agent_system\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 0: Pre-flight\n",
        "bash hpc/00_check_status.sh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Environment setup\n",
        "bash hpc/01_setup_environment.sh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Model download/prep\n",
        "bash hpc/02_download_models.sh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Single participant smoke test\n",
        "bash hpc/03_submit_single.sh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Sequential batch execution\n",
        "bash hpc/04_submit_batch.sh\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Slurm Directives Explained\n",
        "\n",
        "Submission scripts use `#SBATCH` headers. Typical single-GPU fields:\n",
        "\n",
        "- `--partition`: queue/partition name for your cluster.\n",
        "- `--gres=gpu:<type>:1`: request one GPU of a specific type.\n",
        "- `--cpus-per-task`: CPU workers for preprocessing/runtime overhead.\n",
        "- `--mem`: host RAM budget (separate from GPU VRAM).\n",
        "- `--time`: wall-clock cap; job is terminated after this limit.\n",
        "- `--output` / `--error`: stdout/stderr log destinations.\n",
        "\n",
        "If your cluster names resources differently, adapt these values first.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Monitoring and Log Interpretation\n",
        "\n",
        "### Runtime monitoring\n",
        "```bash\n",
        "squeue -u $USER\n",
        "```\n",
        "\n",
        "### Live logs\n",
        "```bash\n",
        "tail -f logs/compass_single_<JOBID>.out\n",
        "tail -f logs/compass_batch_<JOBID>.out\n",
        "```\n",
        "\n",
        "### Error logs\n",
        "```bash\n",
        "cat logs/compass_single_<JOBID>.err\n",
        "cat logs/compass_batch_<JOBID>.err\n",
        "```\n",
        "\n",
        "Notes:\n",
        "- `.out` captures runtime milestones and tool/agent progress.\n",
        "- `.err` captures exceptions/tracebacks and scheduler/runtime failures.\n",
        "- Inspect logs with `cat`, `less`, or `tail`; do not execute them as scripts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Troubleshooting Playbook\n",
        "\n",
        "### Job exits immediately\n",
        "- Check path assumptions (`PROJECT_DIR`, `MODELS_DIR`, `VENV_DIR`, container path).\n",
        "- Run `00_check_status.sh` again.\n",
        "\n",
        "### `apptainer` command missing\n",
        "- Verify cluster module/environment policy for compute nodes.\n",
        "- Confirm script is actually running on compute node, not login node.\n",
        "\n",
        "### Model initialization failure\n",
        "- Confirm model directories are complete and readable.\n",
        "- Check quantization/runtime flags in logs.\n",
        "- Reduce context window if memory pressure appears.\n",
        "\n",
        "### JSON/structured-output instability in local model\n",
        "- Lower generation temperature.\n",
        "- Tighten output token limits for tool calls.\n",
        "- Keep retry/repair strategy enabled in orchestrated flow.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Performance and Stability Tuning\n",
        "\n",
        "Primary levers:\n",
        "\n",
        "- context size (`--max_tokens`, local max model len),\n",
        "- per-role token budgets (`--max_agent_input`, `--max_agent_output`, `--max_tool_input`, `--max_tool_output`),\n",
        "- runtime backend flags (local engine, quantization, eager mode, memory utilization),\n",
        "- chunking and evidence extraction strategy.\n",
        "\n",
        "Guideline:\n",
        "- First optimize for **successful deterministic completion**.\n",
        "- Then optimize latency while keeping output quality acceptable.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Reproducibility and Validation\n",
        "\n",
        "Best practices:\n",
        "\n",
        "- Keep scripts and config under version control.\n",
        "- Capture exact script/runtime info in `.out` logs (already implemented in scripts).\n",
        "- Keep per-participant artifacts in `results/` for auditability.\n",
        "- Validate with fixed cohorts and compare confusion-matrix metrics across runs.\n",
        "- Change one parameter family at a time when benchmarking.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Adaptation Checklist for Other Clusters\n",
        "\n",
        "Before first run on a new HPC:\n",
        "\n",
        "1. Update Slurm directives (`partition`, GPU resource syntax, memory/time limits).\n",
        "2. Update path variables in `hpc/03_submit_single.sh` and `hpc/04_submit_batch.sh`.\n",
        "3. Confirm data root and participant folder conventions.\n",
        "4. Validate `00 -> 03` end-to-end before any `04` batch run.\n",
        "5. Document your site-specific overrides in a local runbook.\n",
        "\n",
        "Related docs:\n",
        "- `hpc/README.md`\n",
        "- `utils/batch_run.py`\n",
        "- `main.py`\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
