{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ğŸ§  COMPASS Explainability Suite\n",
                "## Three XAI Methods for Neuropsychiatric Phenotype Prediction\n",
                "\n",
                "---\n",
                "\n",
                "> **System**: Clinical Ontology-driven Multi-modal Predictive Agentic Support System (COMPASS)  \n",
                "> **Lab**: Computational Neuroimaging Lab, IIS Biobizkaia  \n",
                "> **Author**: Stijn Van Severen\n",
                "\n",
                "---\n",
                "\n",
                "## Why Explainability Is Non-Negotiable in Clinical AI\n",
                "\n",
                "COMPASS predicts neuropsychiatric phenotypes (e.g., Major Depressive Disorder) from high-dimensional, multi-modal brain and clinical data. The system produces a binary classification â€” **CASE vs. CONTROL** â€” together with a probability score. But in clinical and research contexts, *a number alone is never enough*.\n",
                "\n",
                "### The Research Case\n",
                "- **Hypothesis generation**: Which biomarkers actually drive the prediction? Explainability turns a black-box score into a ranked list of candidate biomarkers for follow-up studies.\n",
                "- **Cross-modal validation**: Does the model rely on neuroimaging, lipidomics, or clinical history? Explainability reveals whether the model's reasoning aligns with domain knowledge.\n",
                "- **Reproducibility**: Shapley values and gradient attributions are mathematically grounded â€” they can be reported, compared across cohorts, and audited.\n",
                "\n",
                "### The Clinical Case\n",
                "- **Clinician trust**: A clinician cannot act on an unexplained probability. Feature attributions translate model reasoning into the language of clinical features.\n",
                "- **EU MDR compliance**: Under the EU AI Act and MDR 2017/745, high-risk AI systems in healthcare must provide meaningful explanations. Explainability is a regulatory requirement, not a nice-to-have.\n",
                "- **Error detection**: If the model assigns high importance to a spurious feature (e.g., a data artefact), explainability makes this visible before it causes harm.\n",
                "- **Personalised medicine**: Per-patient attribution maps show *which features matter for this individual*, enabling targeted clinical follow-up.\n",
                "\n",
                "---\n",
                "\n",
                "## The Three XAI Methods in COMPASS\n",
                "\n",
                "COMPASS implements three complementary, LLM-optimised explainability methods â€” each probing a different aspect of the prediction:\n",
                "\n",
                "| Method | File | Role | Signal source | Paper |\n",
                "|--------|------|------|--------------|-------|\n",
                "| **aHFR-TokenSHAP** | `aHFR_TokenSHAP.py` | External / black-box | LLM prediction calls | Extension of [TokenSHAP (Goldshmidt et al., 2024)](https://arxiv.org/abs/2407.10114) |\n",
                "| **Integrated Gradients** | `ig_attribution.py` | Internal / white-box | Local model gradients | [Sundararajan et al., 2017](https://arxiv.org/abs/1703.01365) |\n",
                "| **LLM-Select** | `LLM_select.py` | Hybrid / knowledge-based | LLM domain knowledge | [Inspired by LLM-Select (Enguehard, 2024)](https://arxiv.org/abs/2407.02694) |\n",
                "\n",
                "Together they form a **triangulation strategy**: if all three methods agree on a feature's importance, confidence is high; disagreements reveal interesting model behaviour worth investigating.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ğŸ“¦ Setup & Imports\n",
                "\n",
                "Run this cell first. It adds the COMPASS root to the Python path and imports all necessary modules."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e8717909",
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "import json\n",
                "import math\n",
                "import random\n",
                "import warnings\n",
                "import importlib.util\n",
                "from pathlib import Path\n",
                "from typing import Dict, List, Set, Any, Optional\n",
                "\n",
                "# â”€â”€ Resolve paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "# This notebook lives at: multi_agent_system/utils/validation/explainable_AI/\n",
                "# COMPASS root (multi_agent_system/) is 4 levels up from the notebook file.\n",
                "XAI_DIR      = Path(os.path.abspath(''))\n",
                "COMPASS_ROOT = XAI_DIR.parents[3]   # .../multi_agent_system/\n",
                "INFERENCE_ROOT = COMPASS_ROOT.parent  # .../INFERENCE_PIPELINE/\n",
                "\n",
                "# Add INFERENCE_PIPELINE to sys.path so 'multi_agent_system' is importable\n",
                "if str(INFERENCE_ROOT) not in sys.path:\n",
                "    sys.path.insert(0, str(INFERENCE_ROOT))\n",
                "\n",
                "print(f\"COMPASS root     : {COMPASS_ROOT}\")\n",
                "print(f\"XAI dir          : {XAI_DIR}\")\n",
                "print(f\"sys.path[0]      : {sys.path[0]}\")\n",
                "\n",
                "# â”€â”€ Core COMPASS utilities â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "from multi_agent_system.utils.core.explainability_feature_space import (\n",
                "    build_feature_space,\n",
                "    normalize_abs,\n",
                "    aggregate_leaf_scores_to_parent,\n",
                ")\n",
                "from multi_agent_system.utils.core.explainability_prompt_builder import (\n",
                "    build_prompt_and_spans,\n",
                "    build_prompt_only,\n",
                ")\n",
                "\n",
                "# â”€â”€ XAI module loader â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "# IMPORTANT: we must register the module in sys.modules BEFORE exec_module.\n",
                "# aHFR_TokenSHAP.py uses @dataclass(frozen=True), which calls\n",
                "# sys.modules[cls.__module__].__dict__ during class creation.\n",
                "# Without pre-registration, this raises AttributeError on Python 3.10.\n",
                "\n",
                "def load_xai_module(filename: str):\n",
                "    module_name = filename.replace('.py', '')\n",
                "    path = XAI_DIR / filename\n",
                "    spec = importlib.util.spec_from_file_location(module_name, path)\n",
                "    mod  = importlib.util.module_from_spec(spec)\n",
                "    sys.modules[module_name] = mod   # register BEFORE exec\n",
                "    spec.loader.exec_module(mod)\n",
                "    return mod\n",
                "\n",
                "ahfr      = load_xai_module('aHFR_TokenSHAP.py')\n",
                "ig_mod    = load_xai_module('ig_attribution.py')\n",
                "llmselect = load_xai_module('LLM_select.py')\n",
                "\n",
                "print(\"\\nâœ… All modules loaded successfully.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "bb0d3129",
            "metadata": {},
            "source": [
                "## ğŸ—‚ï¸ Load Pseudo-Patient Data\n",
                "\n",
                "We use **SUBJ_001_PSEUDO** â€” a synthetic patient with features consistent with Major Depressive Disorder.  \n",
                "The data lives in `data/pseudo_data/inputs/SUBJ_001_PSEUDO/` and contains:\n",
                "- `multimodal_data.json` â€” hierarchical multi-modal biomarker data (lipidomics, neuroimaging, etc.)\n",
                "- `non_numerical_data.txt` â€” free-text clinical notes\n",
                "\n",
                "The `build_feature_space()` function converts these into a flat leaf catalogue + hierarchy tree that all three XAI methods consume."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8627e1a4",
            "metadata": {},
            "outputs": [],
            "source": [
                "SUBJ_DIR = COMPASS_ROOT / 'data' / 'pseudo_data' / 'inputs' / 'SUBJ_001_PSEUDO'\n",
                "\n",
                "with open(SUBJ_DIR / 'multimodal_data.json') as f:\n",
                "    multimodal_data = json.load(f)\n",
                "\n",
                "with open(SUBJ_DIR / 'non_numerical_data.txt') as f:\n",
                "    non_numerical_text = f.read()\n",
                "\n",
                "# â”€â”€ Build the shared feature space â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "feature_space = build_feature_space(\n",
                "    multimodal_data,\n",
                "    non_numerical_text=non_numerical_text,\n",
                ")\n",
                "\n",
                "leaf_nodes      = feature_space['leaf_nodes']\n",
                "parent_nodes    = feature_space['parent_nodes']\n",
                "leaf_to_feature = feature_space['leaf_to_feature']\n",
                "leaf_to_parent  = feature_space['leaf_to_parent']\n",
                "hierarchy       = feature_space['hierarchy_children']\n",
                "root_node       = feature_space['root_node']\n",
                "\n",
                "print(f\"Root node    : {root_node}\")\n",
                "print(f\"Leaf features: {len(leaf_nodes)}\")\n",
                "print(f\"Parent nodes : {len(parent_nodes)}\")\n",
                "print(f\"Domains      : {[p for p in parent_nodes if p.startswith('dom::')]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dae329e3",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Method 1 â€” aHFR-TokenSHAP (External / Black-Box)\n",
                "\n",
                "### What it is\n",
                "\n",
                "**aHFR-TokenSHAP** (*Adaptive Hierarchical Frontier Randomisation â€” TokenSHAP*) is a Monte Carlo Shapley estimator designed specifically for LLM-based classifiers. It extends the original [TokenSHAP paper (Goldshmidt et al., 2024)](https://arxiv.org/abs/2407.10114) with three key innovations:\n",
                "\n",
                "1. **Hierarchical frontier sampling** â€” instead of permuting individual features, it samples *mixed-depth frontiers* of the ontology tree (blocks can be whole domains, sub-domains, or individual leaves). This dramatically reduces the number of LLM calls needed for convergence.\n",
                "2. **Adaptive calibration** â€” an initial calibration phase estimates primary-domain importance, then biases subsequent sampling toward informative regions of the hierarchy.\n",
                "3. **Epoch-based weight updates** â€” sampling weights are updated between epochs using cumulative attribution evidence, making the estimator progressively more efficient.\n",
                "\n",
                "### How it works in COMPASS\n",
                "\n",
                "The **score function** is: for a given subset of active leaf features, build a prompt with the rest ablated to `value=__MISSING__`, call the COMPASS predictor LLM, and return `logit(P(CASE))`. The Shapley value of each leaf is its average marginal contribution to this score across many random permutations.\n",
                "\n",
                "### Demo (offline â€” no LLM calls)\n",
                "\n",
                "In this demo we use a **synthetic score function** that simulates LLM behaviour based on z-scores. This lets you run the full Shapley computation without API keys."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b1c129e9",
            "metadata": {},
            "outputs": [],
            "source": [
                "# â”€â”€ Synthetic score function (simulates LLM, no API key needed) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "# In production, this calls the COMPASS predictor LLM and returns logit(P(CASE)).\n",
                "# Here we use z-scores as a proxy: features with high |z| contribute more.\n",
                "\n",
                "rng_score = random.Random(42)\n",
                "\n",
                "def synthetic_score_fn(active_leaves: Set[str]) -> float:\n",
                "    \"\"\"Simulated score: logit(P(CASE)) based on z-scores of active features.\"\"\"\n",
                "    if not active_leaves:\n",
                "        return 0.0\n",
                "    total = 0.0\n",
                "    for leaf_id in active_leaves:\n",
                "        feat = leaf_to_feature.get(leaf_id, {})\n",
                "        z = float(feat.get('z_score', feat.get('value_numeric', 0.0)))\n",
                "        total += z\n",
                "    # Add small noise to simulate LLM stochasticity\n",
                "    total += rng_score.gauss(0, 0.05)\n",
                "    # Clamp to a reasonable logit range\n",
                "    return max(-5.0, min(5.0, total / max(1, len(active_leaves)) * 2.0))\n",
                "\n",
                "print(\"Score function defined.\")\n",
                "print(f\"Score (all features active) : {synthetic_score_fn(set(leaf_nodes)):.4f}\")\n",
                "print(f\"Score (no features active)  : {synthetic_score_fn(set()):.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "df53de2b",
            "metadata": {},
            "outputs": [],
            "source": [
                "# â”€â”€ Run aHFR-TokenSHAP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "# K=20 permutations, adaptive mixed-player mode, 2 independent runs for stability.\n",
                "\n",
                "print(\"Running aHFR-TokenSHAP (K=20, runs=2) ...\")\n",
                "\n",
                "mean_phi, std_phi = ahfr.shapley_with_repeats(\n",
                "    score_fn=synthetic_score_fn,\n",
                "    feature_ids=leaf_nodes,\n",
                "    hierarchy_children=hierarchy,\n",
                "    root=root_node,\n",
                "    leaf_ids=leaf_nodes,\n",
                "    K=20,\n",
                "    runs=2,\n",
                "    seed=0,\n",
                "    adaptive_search=True,\n",
                "    mixed_players=True,\n",
                "    verbose=False,\n",
                ")\n",
                "\n",
                "# Normalise to L1 for comparability\n",
                "ahfr_norm   = normalize_abs(mean_phi)\n",
                "ahfr_parent = normalize_abs(aggregate_leaf_scores_to_parent(mean_phi, leaf_to_parent))\n",
                "\n",
                "print(\"\\nâœ… aHFR-TokenSHAP complete.\")\n",
                "print(f\"   Non-zero leaf attributions: {sum(1 for v in ahfr_norm.values() if v > 0)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "038e02e8",
            "metadata": {},
            "outputs": [],
            "source": [
                "# â”€â”€ Display top-20 leaf features â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "print(\"=\" * 70)\n",
                "print(\"aHFR-TokenSHAP  |  Top-20 Leaf Features by Shapley Value\")\n",
                "print(\"=\" * 70)\n",
                "print(f\"{'Rank':<5} {'Score':>8} {'Â±Std':>8}  Feature\")\n",
                "print(\"-\" * 70)\n",
                "\n",
                "top_leaves = sorted(ahfr_norm.items(), key=lambda kv: kv[1], reverse=True)[:20]\n",
                "for rank, (leaf_id, score) in enumerate(top_leaves, 1):\n",
                "    feat = leaf_to_feature.get(leaf_id, {})\n",
                "    name = feat.get('feature_name', leaf_id)[:40]\n",
                "    std  = std_phi.get(leaf_id, 0.0)\n",
                "    print(f\"  {rank:<3} {score:>8.4f} {std:>8.4f}  {name}\")\n",
                "\n",
                "print(\"\\n--- Domain-level aggregation ---\")\n",
                "print(f\"{'Domain':<35} {'Score':>8}\")\n",
                "print(\"-\" * 45)\n",
                "for dom, score in sorted(ahfr_parent.items(), key=lambda kv: kv[1], reverse=True):\n",
                "    dom_clean = dom.replace('dom::', '')\n",
                "    print(f\"  {dom_clean:<33} {score:>8.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5d9b4fd1",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Method 2 â€” Integrated Gradients (Internal / White-Box)\n",
                "\n",
                "### What it is\n",
                "\n",
                "**Integrated Gradients (IG)** ([Sundararajan et al., 2017](https://arxiv.org/abs/1703.01365)) is a gradient-based attribution method with a formal axiomatic foundation. It satisfies two key axioms:\n",
                "\n",
                "- **Sensitivity**: if a feature changes the prediction, it receives non-zero attribution.\n",
                "- **Implementation invariance**: attributions depend only on the function computed, not the implementation.\n",
                "\n",
                "The method integrates gradients along a straight-line path in embedding space from a **baseline** (a prompt with features ablated) to the **input** (the full prompt). The attribution of each token is the dot product of this integrated gradient with the embedding difference.\n",
                "\n",
                "### How it works in COMPASS\n",
                "\n",
                "COMPASS builds a structured prompt where each feature occupies a known character span. IG computes token-level attributions, which are then aggregated to feature-level using the span-to-token mapping. The score function is `logP(CASE) - logP(CONTROL)` from a **local** causal language model (default: `distilgpt2`).\n",
                "\n",
                "This method is **model-internal**: it probes the local model's embedding space, not the COMPASS LLM. It is fast (no API calls), deterministic, and provides a completeness guarantee: `sum(attributions) â‰ˆ f(input) - f(baseline)`.\n",
                "\n",
                "> **Note**: Loading a local model requires `torch` and `transformers`. The first run downloads `distilgpt2` (~350 MB). Subsequent runs use the cache."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "86a9c374",
            "metadata": {},
            "outputs": [],
            "source": [
                "# â”€â”€ Load local model for IG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "# Default: distilgpt2 (fast, small). Change model_name for a larger model.\n",
                "\n",
                "IG_MODEL = 'distilgpt2'\n",
                "\n",
                "print(f\"Loading model: {IG_MODEL} ...\")\n",
                "model, tokenizer, device = ig_mod.load_model(model_name=IG_MODEL)\n",
                "labels = ig_mod.prepare_label_tokens(tokenizer, case_str=' CASE', control_str=' CONTROL')\n",
                "\n",
                "print(f\"  Device       : {device}\")\n",
                "print(f\"  Label tokens : CASE={labels.case_ids}  CONTROL={labels.control_ids}\")\n",
                "print(f\"  Single-token : {labels.single_token_labels}\")\n",
                "print(\"\\nâœ… Model ready.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "91506dd2",
            "metadata": {},
            "outputs": [],
            "source": [
                "# â”€â”€ Build the structured prompt with character spans â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "TARGET  = 'Major Depressive Disorder'\n",
                "CONTROL = 'brain-implicated pathology, but NOT psychiatric'\n",
                "\n",
                "prompt, spans = build_prompt_and_spans(\n",
                "    target_condition=TARGET,\n",
                "    control_condition=CONTROL,\n",
                "    leaf_features=leaf_to_feature,\n",
                "    active_leaf_ids=set(leaf_nodes),\n",
                ")\n",
                "\n",
                "print(f\"Prompt length : {len(prompt)} characters\")\n",
                "print(f\"Feature spans : {len(spans)} features mapped\")\n",
                "print(\"\\n--- Prompt preview (first 600 chars) ---\")\n",
                "print(prompt[:600])\n",
                "print(\"...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "908a4739",
            "metadata": {},
            "outputs": [],
            "source": [
                "# â”€â”€ Run Integrated Gradients â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "# steps=8 is fast for demonstration; use steps=50+ for publication-quality results.\n",
                "\n",
                "print(\"Running Integrated Gradients (steps=8, baseline=mask) ...\")\n",
                "print(\"(This may take 30â€“120 s on CPU depending on prompt length)\")\n",
                "\n",
                "ig_scores, ig_debug = ig_mod.integrated_gradients_feature_importance(\n",
                "    model=model,\n",
                "    tokenizer=tokenizer,\n",
                "    prompt=prompt,\n",
                "    feature_spans=spans,\n",
                "    labels=labels,\n",
                "    device=device,\n",
                "    steps=8,\n",
                "    baseline_mode='mask',   # mask feature VALUE tokens â†’ stable alignment\n",
                "    span_mode='value',      # attribute only the value text, not the full line\n",
                "    check_completeness=True,\n",
                "    return_debug=True,\n",
                ")\n",
                "\n",
                "ig_norm   = normalize_abs(ig_scores)\n",
                "ig_parent = normalize_abs(aggregate_leaf_scores_to_parent(ig_scores, leaf_to_parent))\n",
                "\n",
                "print(\"\\nâœ… Integrated Gradients complete.\")\n",
                "print(f\"   Prompt tokens        : {int(ig_debug['T_tokens'])}\")\n",
                "print(f\"   Feature tokens union : {int(ig_debug['n_feature_tokens_union'])}\")\n",
                "if 'delta_s' in ig_debug:\n",
                "    print(f\"   Completeness check   : Î”s={ig_debug['delta_s']:+.4f}  \"\n",
                "          f\"Î£attr={ig_debug['total_token_attr']:+.4f}  \"\n",
                "          f\"err={ig_debug['completeness_abs_err']:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7a0667f9",
            "metadata": {},
            "outputs": [],
            "source": [
                "# â”€â”€ Display IG results â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "print(\"=\" * 70)\n",
                "print(\"Integrated Gradients  |  Top-20 Leaf Features\")\n",
                "print(\"=\" * 70)\n",
                "print(f\"{'Rank':<5} {'Score (L1)':>12} {'Raw IG':>10}  Feature\")\n",
                "print(\"-\" * 70)\n",
                "\n",
                "top_ig = sorted(ig_norm.items(), key=lambda kv: kv[1], reverse=True)[:20]\n",
                "for rank, (leaf_id, score) in enumerate(top_ig, 1):\n",
                "    feat = leaf_to_feature.get(leaf_id, {})\n",
                "    name = feat.get('feature_name', leaf_id)[:40]\n",
                "    raw  = ig_scores.get(leaf_id, 0.0)\n",
                "    print(f\"  {rank:<3} {score:>12.6f} {raw:>+10.4f}  {name}\")\n",
                "\n",
                "print(\"\\n--- Domain-level aggregation ---\")\n",
                "for dom, score in sorted(ig_parent.items(), key=lambda kv: kv[1], reverse=True):\n",
                "    dom_clean = dom.replace('dom::', '')\n",
                "    print(f\"  {dom_clean:<35} {score:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "37f0bf2b",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Method 3 â€” LLM-Select (Hybrid / Knowledge-Based)\n",
                "\n",
                "### What it is\n",
                "\n",
                "**LLM-Select** ([Enguehard, 2024](https://arxiv.org/abs/2407.02694)) uses an LLM's **prior domain knowledge** â€” rather than patient-specific data â€” to score feature importance. The LLM is asked: *\"Given the task of predicting [phenotype], how important is each feature?\"* and returns integer scores (1â€“1000) for every leaf and parent feature.\n",
                "\n",
                "### Why this is valuable\n",
                "\n",
                "- **Zero data required**: LLM-Select does not need the patient's actual values. It reflects what the model *knows* about the phenotype from pre-training.\n",
                "- **Sanity check**: If LLM-Select and aHFR-TokenSHAP agree on top features, the model's reasoning is consistent with biomedical literature.\n",
                "- **Prior elicitation**: In small-cohort research, LLM-Select provides a literature-informed prior that can guide feature selection before running expensive experiments.\n",
                "- **Complementarity**: LLM-Select captures *general* feature relevance; aHFR-TokenSHAP captures *patient-specific* relevance. Both are needed.\n",
                "\n",
                "### How it works in COMPASS\n",
                "\n",
                "The method calls an OpenAI-compatible API (OpenRouter or direct OpenAI) with a structured JSON output schema. Multiple repeats are averaged to reduce sampling variance.\n",
                "\n",
                "> **Requires**: `OPENAI_API_KEY` or `OPENROUTER_API_KEY` in your `.env` file.  \n",
                "> The demo below shows the full workflow. If no key is available, a mock result is shown."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "48de6449",
            "metadata": {},
            "outputs": [],
            "source": [
                "# â”€â”€ LLM-Select: check for API key â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "from dotenv import load_dotenv\n",
                "load_dotenv(COMPASS_ROOT / '.env')\n",
                "\n",
                "OPENAI_KEY     = os.environ.get('OPENAI_API_KEY', '')\n",
                "OPENROUTER_KEY = os.environ.get('OPENROUTER_API_KEY', '')\n",
                "\n",
                "HAS_API_KEY = bool(OPENAI_KEY or OPENROUTER_KEY)\n",
                "print(f\"OpenAI key available    : {bool(OPENAI_KEY)}\")\n",
                "print(f\"OpenRouter key available: {bool(OPENROUTER_KEY)}\")\n",
                "\n",
                "if not HAS_API_KEY:\n",
                "    print(\"\\nâš ï¸  No API key found. A mock result will be shown instead.\")\n",
                "    print(\"   Add OPENAI_API_KEY or OPENROUTER_API_KEY to your .env file to run live.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5350149c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# â”€â”€ Run LLM-Select (or mock) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "# Use a small subset of features for the demo to keep the prompt manageable.\n",
                "# In production, all leaf_nodes and parent_nodes are passed.\n",
                "\n",
                "DEMO_LEAF_IDS   = leaf_nodes[:30]   # first 30 leaves for demo\n",
                "DEMO_PARENT_IDS = parent_nodes[:10] # first 10 parents\n",
                "\n",
                "if HAS_API_KEY:\n",
                "    client_kwargs = {}\n",
                "    if OPENROUTER_KEY:\n",
                "        client_kwargs = {\n",
                "            'api_key': OPENROUTER_KEY,\n",
                "            'base_url': 'https://openrouter.ai/api/v1',\n",
                "        }\n",
                "\n",
                "    print(\"Running LLM-Select (repeats=2, model=gpt-4o-mini) ...\")\n",
                "    try:\n",
                "        leaf_runs, parent_runs = llmselect.get_llm_select_scores(\n",
                "            phenotype=TARGET,\n",
                "            leaf_ids=DEMO_LEAF_IDS,\n",
                "            parent_ids=DEMO_PARENT_IDS,\n",
                "            repeats=2,\n",
                "            model='gpt-4o-mini',\n",
                "            temperature=0.7,\n",
                "            client_kwargs=client_kwargs,\n",
                "        )\n",
                "        llmsel_leaf = {\n",
                "            lid: sum(r['norm'].get(lid, 0.0) for r in leaf_runs) / len(leaf_runs)\n",
                "            for lid in DEMO_LEAF_IDS\n",
                "        }\n",
                "        llmsel_parent = {\n",
                "            pid: sum(r['norm'].get(pid, 0.0) for r in parent_runs) / len(parent_runs)\n",
                "            for pid in DEMO_PARENT_IDS\n",
                "        }\n",
                "        print(\"\\nâœ… LLM-Select complete (live API).\")\n",
                "        LLMSEL_LIVE = True\n",
                "    except Exception as e:\n",
                "        print(f\"\\nâš ï¸  LLM-Select API call failed: {e}\")\n",
                "        LLMSEL_LIVE = False\n",
                "else:\n",
                "    LLMSEL_LIVE = False\n",
                "\n",
                "if not LLMSEL_LIVE:\n",
                "    # â”€â”€ Mock: simulate LLM scores using z-score magnitude as proxy â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "    print(\"Generating mock LLM-Select scores (z-score proxy) ...\")\n",
                "    rng_mock = random.Random(99)\n",
                "    llmsel_leaf = {}\n",
                "    for lid in DEMO_LEAF_IDS:\n",
                "        feat = leaf_to_feature.get(lid, {})\n",
                "        z = abs(float(feat.get('z_score', feat.get('value_numeric', 0.0))))\n",
                "        llmsel_leaf[lid] = z + rng_mock.uniform(0, 0.5)\n",
                "    total = sum(llmsel_leaf.values()) or 1.0\n",
                "    llmsel_leaf = {k: v / total for k, v in llmsel_leaf.items()}\n",
                "\n",
                "    llmsel_parent = {}\n",
                "    for pid in DEMO_PARENT_IDS:\n",
                "        llmsel_parent[pid] = rng_mock.uniform(0.05, 0.3)\n",
                "    total_p = sum(llmsel_parent.values()) or 1.0\n",
                "    llmsel_parent = {k: v / total_p for k, v in llmsel_parent.items()}\n",
                "\n",
                "    print(\"âœ… Mock LLM-Select scores generated.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "32181490",
            "metadata": {},
            "outputs": [],
            "source": [
                "# â”€â”€ Display LLM-Select results â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "print(\"=\" * 70)\n",
                "print(f\"LLM-Select  |  Top-20 Leaf Features  {'(LIVE)' if LLMSEL_LIVE else '(MOCK)'}\")\n",
                "print(\"=\" * 70)\n",
                "print(f\"{'Rank':<5} {'Score (L1)':>12}  Feature\")\n",
                "print(\"-\" * 70)\n",
                "\n",
                "top_llmsel = sorted(llmsel_leaf.items(), key=lambda kv: kv[1], reverse=True)[:20]\n",
                "for rank, (leaf_id, score) in enumerate(top_llmsel, 1):\n",
                "    feat = leaf_to_feature.get(leaf_id, {})\n",
                "    name = feat.get('feature_name', leaf_id)[:45]\n",
                "    print(f\"  {rank:<3} {score:>12.6f}  {name}\")\n",
                "\n",
                "print(\"\\n--- Domain-level scores ---\")\n",
                "for dom, score in sorted(llmsel_parent.items(), key=lambda kv: kv[1], reverse=True):\n",
                "    dom_clean = dom.replace('dom::', '')\n",
                "    print(f\"  {dom_clean:<35} {score:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b10d0b83",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ğŸ” Cross-Method Comparison\n",
                "\n",
                "The real power of the COMPASS XAI suite lies in **triangulation**: comparing all three methods reveals where they agree (high-confidence features) and where they diverge (interesting model behaviour).\n",
                "\n",
                "- **Agreement** â†’ feature is robustly important across signal sources\n",
                "- **aHFR high, IG low** â†’ feature matters for LLM reasoning but not for local model gradients (may reflect LLM-specific knowledge)\n",
                "- **LLM-Select high, aHFR low** â†’ feature is known to be relevant in literature but does not drive *this patient's* prediction\n",
                "- **IG high, others low** â†’ feature has strong local embedding signal but may be a prompt artefact"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "41149f94",
            "metadata": {},
            "outputs": [],
            "source": [
                "# â”€â”€ Build comparison table over shared leaf IDs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "shared_ids = [lid for lid in DEMO_LEAF_IDS if lid in ahfr_norm and lid in ig_norm]\n",
                "\n",
                "rows = []\n",
                "for lid in shared_ids:\n",
                "    feat = leaf_to_feature.get(lid, {})\n",
                "    name = feat.get('feature_name', lid)[:35]\n",
                "    a = ahfr_norm.get(lid, 0.0)\n",
                "    b = ig_norm.get(lid, 0.0)\n",
                "    c = llmsel_leaf.get(lid, 0.0)\n",
                "    avg = (a + b + c) / 3.0\n",
                "    rows.append((avg, name, a, b, c))\n",
                "\n",
                "rows.sort(reverse=True)\n",
                "\n",
                "print(\"=\" * 85)\n",
                "print(\"Cross-Method Comparison  |  Shared Features  (sorted by mean score)\")\n",
                "print(\"=\" * 85)\n",
                "print(f\"{'Feature':<37} {'aHFR':>8} {'IG':>8} {'LLMSel':>8} {'Mean':>8}\")\n",
                "print(\"-\" * 85)\n",
                "for avg, name, a, b, c in rows[:25]:\n",
                "    print(f\"  {name:<35} {a:>8.4f} {b:>8.4f} {c:>8.4f} {avg:>8.4f}\")\n",
                "\n",
                "print(\"\\nLegend: aHFR=aHFR-TokenSHAP  IG=Integrated Gradients  LLMSel=LLM-Select\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d2c6a76a",
            "metadata": {},
            "outputs": [],
            "source": [
                "# â”€â”€ Rank correlation between methods â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "# Spearman rank correlation: 1.0 = perfect agreement, 0.0 = no agreement.\n",
                "\n",
                "def spearman_rho(scores_a: dict, scores_b: dict) -> float:\n",
                "    keys = sorted(set(scores_a) & set(scores_b))\n",
                "    if len(keys) < 2:\n",
                "        return float('nan')\n",
                "    def ranks(d):\n",
                "        sorted_keys = sorted(keys, key=lambda k: d.get(k, 0.0), reverse=True)\n",
                "        return {k: i+1 for i, k in enumerate(sorted_keys)}\n",
                "    ra, rb = ranks(scores_a), ranks(scores_b)\n",
                "    n = len(keys)\n",
                "    d2 = sum((ra[k] - rb[k])**2 for k in keys)\n",
                "    return 1.0 - 6.0 * d2 / (n * (n**2 - 1))\n",
                "\n",
                "rho_ahfr_ig     = spearman_rho(ahfr_norm,  ig_norm)\n",
                "rho_ahfr_llmsel = spearman_rho(ahfr_norm,  llmsel_leaf)\n",
                "rho_ig_llmsel   = spearman_rho(ig_norm,    llmsel_leaf)\n",
                "\n",
                "print(\"Spearman Rank Correlation between XAI Methods\")\n",
                "print(\"=\" * 45)\n",
                "print(f\"  aHFR-TokenSHAP  â†”  IG              : {rho_ahfr_ig:+.3f}\")\n",
                "print(f\"  aHFR-TokenSHAP  â†”  LLM-Select      : {rho_ahfr_llmsel:+.3f}\")\n",
                "print(f\"  IG              â†”  LLM-Select       : {rho_ig_llmsel:+.3f}\")\n",
                "print()\n",
                "print(\"Interpretation:\")\n",
                "print(\"  > 0.6  â†’ strong agreement (feature ranking is consistent)\")\n",
                "print(\"  0.3â€“0.6 â†’ moderate agreement\")\n",
                "print(\"  < 0.3  â†’ methods probe different aspects of the model\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c20eb546",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ğŸ”— Integration with the COMPASS Pipeline\n",
                "\n",
                "In production, all three methods are orchestrated automatically by `utils/core/explainability_runner.py`. The runner is invoked from `main.py` via the `--xai_methods` flag:\n",
                "\n",
                "```bash\n",
                "python main.py data/pseudo_data/inputs/SUBJ_001_PSEUDO \\\n",
                "  --target \"Major Depressive Disorder\" \\\n",
                "  --control \"brain-implicated pathology, but NOT psychiatric\" \\\n",
                "  --backend openrouter \\\n",
                "  --xai_methods external,internal,hybrid\n",
                "```\n",
                "\n",
                "The runner:\n",
                "1. Calls `build_feature_space()` to create the shared leaf catalogue and hierarchy.\n",
                "2. Selects the best COMPASS prediction attempt (highest critic score).\n",
                "3. Runs each requested method in sequence.\n",
                "4. Saves results to `xai_feature_importance_{participant_id}.json`.\n",
                "5. Passes results to the **Communicator** agent, which generates a human-readable `xai_explainability_report.md`.\n",
                "\n",
                "The cell below demonstrates the data flow using the pseudo-patient data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a5f3241c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# â”€â”€ COMPASS runner data flow summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
                "print(\"COMPASS XAI Runner â€” Data Flow Summary\")\n",
                "print(\"=\" * 60)\n",
                "print()\n",
                "print(\"Step 1: build_feature_space()\")\n",
                "print(f\"  Input  : multimodal_data.json + non_numerical_data.txt\")\n",
                "print(f\"  Output : {len(leaf_nodes)} leaf features, {len(parent_nodes)} parent nodes\")\n",
                "print()\n",
                "print(\"Step 2: run_explainability_methods()\")\n",
                "print(\"  external â†’ aHFR_TokenSHAP.py  (monte_carlo_hfr_tokenshap / shapley_with_repeats)\")\n",
                "print(\"  internal â†’ ig_attribution.py  (integrated_gradients_feature_importance)\")\n",
                "print(\"  hybrid   â†’ LLM_select.py      (get_llm_select_scores)\")\n",
                "print()\n",
                "print(\"Step 3: Output artifacts\")\n",
                "print(\"  xai_feature_importance_{participant_id}.json\")\n",
                "print(\"  xai_explainability_report.md  (generated by Communicator agent)\")\n",
                "print()\n",
                "print(\"Feature space summary for SUBJ_001_PSEUDO:\")\n",
                "domain_counts = {}\n",
                "for lid in leaf_nodes:\n",
                "    dom = leaf_to_feature.get(lid, {}).get('domain', 'UNKNOWN')\n",
                "    domain_counts[dom] = domain_counts.get(dom, 0) + 1\n",
                "for dom, cnt in sorted(domain_counts.items(), key=lambda x: -x[1]):\n",
                "    print(f\"  {dom:<35} {cnt:>4} features\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "61d62e52",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ğŸ“‹ Summary & Method Comparison\n",
                "\n",
                "| Property | aHFR-TokenSHAP | Integrated Gradients | LLM-Select |\n",
                "|----------|---------------|---------------------|------------|\n",
                "| **Signal source** | LLM prediction calls | Local model gradients | LLM prior knowledge |\n",
                "| **Requires API** | Yes (predictor LLM) | No (local model) | Yes (scoring LLM) |\n",
                "| **Patient-specific** | âœ… Yes | âœ… Yes | âŒ No (phenotype-level) |\n",
                "| **Theoretical basis** | Shapley values (game theory) | Axiomatic attribution | LLM knowledge elicitation |\n",
                "| **Hierarchy-aware** | âœ… Yes (HFR) | âŒ No (token-level) | âœ… Yes (parent + leaf) |\n",
                "| **Computational cost** | High (many LLM calls) | Medium (local fwd/bwd) | Low (few LLM calls) |\n",
                "| **Key parameter** | K (permutations) | steps (integration steps) | repeats |\n",
                "| **Output** | Leaf Shapley values Â± std | Token/feature attributions | Integer importance scores |\n",
                "| **Completeness axiom** | âœ… (in expectation) | âœ… (exact) | âŒ (heuristic) |\n",
                "\n",
                "### Recommended usage\n",
                "\n",
                "- **Research**: Run all three. Report Spearman correlation as a consistency metric. Highlight features where all three agree.\n",
                "- **Clinical report**: Use aHFR-TokenSHAP (patient-specific, LLM-grounded) as the primary attribution. Use LLM-Select as a literature-informed sanity check.\n",
                "- **Debugging**: Use IG to inspect whether the local model's embedding space encodes clinically meaningful signals.\n",
                "\n",
                "---\n",
                "\n",
                "## ğŸ“š References\n",
                "\n",
                "1. **TokenSHAP**: Goldshmidt, R., et al. (2024). *TokenSHAP: Interpreting Large Language Models with Monte Carlo Shapley Value Estimation*. arXiv:2407.10114. https://arxiv.org/abs/2407.10114\n",
                "\n",
                "2. **Integrated Gradients**: Sundararajan, M., Taly, A., & Yan, Q. (2017). *Axiomatic Attribution for Deep Networks*. ICML 2017. arXiv:1703.01365. https://arxiv.org/abs/1703.01365\n",
                "\n",
                "3. **LLM-Select**: Enguehard, J. (2024). *LLM-Select: Feature Selection with Large Language Models*. arXiv:2407.02694. https://arxiv.org/abs/2407.02694\n",
                "\n",
                "4. **COMPASS-engine**: Van Severen, S., Diez, I. P., & Cortes, J. M. (2026). *COMPASS: Clinical ontology-driven multi-modal predictive agentic support system*. Manuscript in preparation."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
