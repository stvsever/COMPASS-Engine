{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPASS Validation Guide (Annotated Datasets)\n",
    "\n",
    "This notebook is a **didactic, mode-specific guide** for automated annotation validation in COMPASS.\n",
    "\n",
    "Covered prediction types:\n",
    "\n",
    "- binary classification\n",
    "- multiclass classification\n",
    "- univariate regression\n",
    "- multivariate regression\n",
    "- hierarchical mixed-task prediction\n",
    "\n",
    "All commands assume project root: `multi_agent_system/`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Validation Workflow (Common to All Modes)\n",
    "\n",
    "Validation runs in two layers:\n",
    "\n",
    "1. `run_validation_metrics.py`\n",
    "   - computes core metrics + core plots\n",
    "2. `detailed_analysis.py`\n",
    "   - computes extended diagnostics + row-level audit artifacts\n",
    "\n",
    "Recommended review order:\n",
    "\n",
    "1. annotation contract artifacts (`annotation_contract*`)\n",
    "2. core metrics JSON (`*_metrics.json`)\n",
    "3. mode-specific plots\n",
    "4. detailed rows + detailed text reports\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inputs and Template Files\n",
    "\n",
    "Required inputs:\n",
    "\n",
    "- participant outputs in `results/participant_runs/`\n",
    "- binary mode: `--targets_file` (JSON only)\n",
    "- non-binary modes: `--annotations_json`\n",
    "\n",
    "Template examples:\n",
    "\n",
    "- `utils/validation/with_annotated_dataset/annotation_templates/examples/binary_targets_example.json`\n",
    "- `utils/validation/with_annotated_dataset/annotation_templates/examples/multiclass_annotations_example.json`\n",
    "- `utils/validation/with_annotated_dataset/annotation_templates/examples/regression_univariate_annotations_example.json`\n",
    "- `utils/validation/with_annotated_dataset/annotation_templates/examples/regression_multivariate_annotations_example.json`\n",
    "- `utils/validation/with_annotated_dataset/annotation_templates/examples/hierarchical_annotations_example.json`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Binary Classification Validation\n",
    "\n",
    "### Automated contract checks\n",
    "\n",
    "- binary annotation JSON parses correctly\n",
    "- participant ID alignment (annotation â†” participant output)\n",
    "- valid binary truth labels (`CASE` / `CONTROL`)\n",
    "- extractable binary prediction payload\n",
    "\n",
    "### Automated outputs (core)\n",
    "\n",
    "- `binary_metrics_integrated.json`\n",
    "- `integrated_confusion_matrix.png`\n",
    "\n",
    "### Automated outputs (detailed)\n",
    "\n",
    "- `detailed_analysis.txt`\n",
    "- `detailed_analysis_binary.json`\n",
    "- binary diagnostics plots (composite vs correctness, calibration, verdict accuracy, iteration improvement; availability depends on data support)\n",
    "\n",
    "### How to interpret\n",
    "\n",
    "- start with confusion matrix and sensitivity/specificity\n",
    "- use Brier/ECE + calibration for probability reliability\n",
    "- inspect verdict/composite diagnostics for critic-quality alignment\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Binary validation\n",
    "!python utils/validation/with_annotated_dataset/run_validation_metrics.py \\\n",
    "    --results_dir ../results/participant_runs \\\n",
    "    --prediction_type binary \\\n",
    "    --targets_file ../data/__TARGETS__/binary_targets.json \\\n",
    "    --output_dir ../results/analysis/binary_confusion_matrix \\\n",
    "    --disorder_groups \"MAJOR_DEPRESSIVE_DISORDER,ANXIETY_DISORDERS\"\n",
    "\n",
    "!python utils/validation/with_annotated_dataset/detailed_analysis.py \\\n",
    "    --results_dir ../results/participant_runs \\\n",
    "    --prediction_type binary \\\n",
    "    --targets_file ../data/__TARGETS__/binary_targets.json \\\n",
    "    --output_dir ../results/analysis/details \\\n",
    "    --disorder_groups \"MAJOR_DEPRESSIVE_DISORDER,ANXIETY_DISORDERS\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multiclass Classification Validation\n",
    "\n",
    "### Automated contract checks\n",
    "\n",
    "- class label presence per participant\n",
    "- participant ID alignment\n",
    "- valid prediction extraction for multiclass payload\n",
    "\n",
    "### Automated outputs (core)\n",
    "\n",
    "- `multiclass_metrics.json`\n",
    "- `annotation_contract_multiclass.json`\n",
    "- multiclass confusion matrix / per-class metrics / calibration plots\n",
    "\n",
    "### Automated outputs (detailed)\n",
    "\n",
    "- `detailed_analysis_multiclass.json`\n",
    "- `detailed_analysis_multiclass.txt`\n",
    "- `detailed_rows_multiclass.json`\n",
    "- `detailed_annotation_contract_multiclass.json/.txt`\n",
    "- extended diagnostics (top confusions, confidence/entropy, label distribution)\n",
    "\n",
    "### How to interpret\n",
    "\n",
    "- read macro F1 first, then per-class F1\n",
    "- inspect top confusions for error structure\n",
    "- use confidence diagnostics to detect overconfident mistakes\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Multiclass validation\n",
    "!python utils/validation/with_annotated_dataset/run_validation_metrics.py \\\n",
    "    --results_dir ../results/participant_runs \\\n",
    "    --prediction_type multiclass \\\n",
    "    --annotations_json ../data/__TARGETS__/annotated_targets.json \\\n",
    "    --output_dir ../results/analysis/multiclass\n",
    "\n",
    "!python utils/validation/with_annotated_dataset/detailed_analysis.py \\\n",
    "    --results_dir ../results/participant_runs \\\n",
    "    --prediction_type multiclass \\\n",
    "    --annotations_json ../data/__TARGETS__/annotated_targets.json \\\n",
    "    --output_dir ../results/analysis/multiclass_details\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Univariate Regression Validation\n",
    "\n",
    "### Automated contract checks\n",
    "\n",
    "- exactly one numeric target output per participant\n",
    "- participant ID alignment\n",
    "- extractable univariate regression prediction\n",
    "\n",
    "### Automated outputs (core)\n",
    "\n",
    "- `regression_univariate_metrics.json`\n",
    "- `annotation_contract_regression_univariate.json`\n",
    "- parity + error bar plots\n",
    "\n",
    "### Automated outputs (detailed)\n",
    "\n",
    "- `detailed_analysis_regression_univariate.json/.txt`\n",
    "- `detailed_rows_regression_univariate.json`\n",
    "- `detailed_annotation_contract_regression_univariate.json/.txt`\n",
    "- residual distribution, residual-vs-true, top-error diagnostics\n",
    "\n",
    "### How to interpret\n",
    "\n",
    "- MAE/RMSE = scale of error\n",
    "- R2 = explained variance quality\n",
    "- residual diagnostics = bias/heteroscedasticity structure\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Univariate regression validation\n",
    "!python utils/validation/with_annotated_dataset/run_validation_metrics.py \\\n",
    "    --results_dir ../results/participant_runs \\\n",
    "    --prediction_type regression_univariate \\\n",
    "    --annotations_json ../data/__TARGETS__/annotated_targets.json \\\n",
    "    --output_dir ../results/analysis/univariate_regression\n",
    "\n",
    "!python utils/validation/with_annotated_dataset/detailed_analysis.py \\\n",
    "    --results_dir ../results/participant_runs \\\n",
    "    --prediction_type regression_univariate \\\n",
    "    --annotations_json ../data/__TARGETS__/annotated_targets.json \\\n",
    "    --output_dir ../results/analysis/univariate_regression_details\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multivariate Regression Validation\n",
    "\n",
    "### Automated contract checks\n",
    "\n",
    "- two or more numeric outputs per participant\n",
    "- participant ID alignment\n",
    "- extractable multivariate regression prediction\n",
    "\n",
    "### Automated outputs (core)\n",
    "\n",
    "- `regression_multivariate_metrics.json`\n",
    "- `annotation_contract_regression_multivariate.json`\n",
    "- parity + error bar plots\n",
    "\n",
    "### Automated outputs (detailed)\n",
    "\n",
    "- `detailed_analysis_regression_multivariate.json/.txt`\n",
    "- `detailed_rows_regression_multivariate.json`\n",
    "- `detailed_annotation_contract_regression_multivariate.json/.txt`\n",
    "- residual diagnostics + top absolute error ranking\n",
    "\n",
    "### How to interpret\n",
    "\n",
    "- inspect per-output metrics before macro summary\n",
    "- use top-error ranking to identify problematic output dimensions\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Multivariate regression validation\n",
    "!python utils/validation/with_annotated_dataset/run_validation_metrics.py \\\n",
    "    --results_dir ../results/participant_runs \\\n",
    "    --prediction_type regression_multivariate \\\n",
    "    --annotations_json ../data/__TARGETS__/annotated_targets.json \\\n",
    "    --output_dir ../results/analysis/multivariate_regression\n",
    "\n",
    "!python utils/validation/with_annotated_dataset/detailed_analysis.py \\\n",
    "    --results_dir ../results/participant_runs \\\n",
    "    --prediction_type regression_multivariate \\\n",
    "    --annotations_json ../data/__TARGETS__/annotated_targets.json \\\n",
    "    --output_dir ../results/analysis/multivariate_regression_details\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hierarchical Mixed-Task Validation\n",
    "\n",
    "### Automated contract checks (strict)\n",
    "\n",
    "- node payload validity per participant\n",
    "- **cross-participant schema consistency**:\n",
    "  - same node IDs\n",
    "  - same node mode per node ID\n",
    "  - same regression output-key set per regression node\n",
    "\n",
    "If this consistency is violated, validation stops with an explicit schema mismatch error.\n",
    "\n",
    "### Schema consistency checklist\n",
    "\n",
    "1. every participant has the same node set\n",
    "2. each node keeps the same mode in all rows\n",
    "3. regression node output keys are identical across rows\n",
    "4. classification nodes always include a label\n",
    "5. regression nodes always include numeric values\n",
    "\n",
    "### Automated outputs (core)\n",
    "\n",
    "- `hierarchical_metrics.json`\n",
    "- `annotation_contract_hierarchical.json`\n",
    "- node score/support + node-type distribution plots\n",
    "\n",
    "### Automated outputs (detailed)\n",
    "\n",
    "- `detailed_analysis_hierarchical.json/.txt`\n",
    "- `detailed_rows_hierarchical.json`\n",
    "- `detailed_annotation_contract_hierarchical.json/.txt`\n",
    "- node coverage + hierarchical metric heatmap diagnostics\n",
    "\n",
    "### How to interpret\n",
    "\n",
    "- inspect per-node metrics before macro score\n",
    "- inspect coverage and support before concluding model quality\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Hierarchical mixed-task validation\n",
    "!python utils/validation/with_annotated_dataset/run_validation_metrics.py \\\n",
    "    --results_dir ../results/participant_runs \\\n",
    "    --prediction_type hierarchical \\\n",
    "    --annotations_json ../data/__TARGETS__/annotated_targets.json \\\n",
    "    --output_dir ../results/analysis/hierarchical\n",
    "\n",
    "!python utils/validation/with_annotated_dataset/detailed_analysis.py \\\n",
    "    --results_dir ../results/participant_runs \\\n",
    "    --prediction_type hierarchical \\\n",
    "    --annotations_json ../data/__TARGETS__/annotated_targets.json \\\n",
    "    --output_dir ../results/analysis/hierarchical_details\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cross-Mode Artifact Inspection Helper\n",
    "\n",
    "Use this helper to quickly inspect metrics, contract validity, and top annotation issues across generated outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Quick artifact inspection helper (metrics + contract + detailed rows)\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "out_dir = Path('../results/analysis')\n",
    "\n",
    "print('\\n[METRICS]')\n",
    "for p in sorted(out_dir.rglob('*_metrics.json'))[:50]:\n",
    "    payload = json.loads(p.read_text())\n",
    "    ptype = payload.get('prediction_type')\n",
    "    n_rows = payload.get('n_rows')\n",
    "    contract = payload.get('annotation_contract') if isinstance(payload, dict) else None\n",
    "    valid = contract.get('n_valid_rows') if isinstance(contract, dict) else None\n",
    "    total = contract.get('n_rows') if isinstance(contract, dict) else None\n",
    "    if valid is not None:\n",
    "        print(f'- {p.name}: type={ptype} rows={n_rows} contract={valid}/{total}')\n",
    "    else:\n",
    "        print(f'- {p.name}: type={ptype} rows={n_rows}')\n",
    "\n",
    "print('\\n[ANNOTATION CONTRACT ISSUES]')\n",
    "for p in sorted(out_dir.rglob('annotation_contract_*.json'))[:50]:\n",
    "    payload = json.loads(p.read_text())\n",
    "    issues = payload.get('issue_counts') if isinstance(payload.get('issue_counts'), dict) else {}\n",
    "    top = sorted(issues.items(), key=lambda kv: int(kv[1] or 0), reverse=True)[:5]\n",
    "    top_str = ', '.join([f'{k}={v}' for k, v in top]) if top else 'none'\n",
    "    print(f\"- {p.name}: valid={payload.get('n_valid_rows')}/{payload.get('n_rows')}; top_issues={top_str}\")\n",
    "\n",
    "print('\\n[DETAILED ROW PAYLOADS]')\n",
    "for p in sorted(out_dir.rglob('detailed_rows_*.json'))[:50]:\n",
    "    payload = json.loads(p.read_text())\n",
    "    print(f\"- {p.name}: n_rows={payload.get('n_rows')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. HPC and Batch Integration Notes\n",
    "\n",
    "- `hpc/05_submit_batch.sh` forwards `PREDICTION_TYPE` to validation scripts.\n",
    "- Binary validation expects JSON `TARGETS_FILE`.\n",
    "- Non-binary validation expects `ANNOTATIONS_JSON`.\n",
    "- The same annotation-contract logic is shared across local, batch, and HPC workflows.\n",
    "\n",
    "Example:\n",
    "\n",
    "```bash\n",
    "PREDICTION_TYPE=regression_univariate \\\n",
    "ANNOTATIONS_JSON=~/compass_pipeline/data/__TARGETS__/annotated_targets.json \\\n",
    "bash hpc/05_submit_batch.sh\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. XAI Scope\n",
    "\n",
    "XAI methods currently apply only to pure root-level binary classification.\n",
    "\n",
    "For multiclass/regression/hierarchical validation, outputs remain fully supported and include explicit `xai_status: skipped` metadata.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
