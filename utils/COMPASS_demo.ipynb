{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPASS Engine: Interactive Demo\n",
    "\n",
    "## Clinical Orchestrated Multi-modal Predictive Agentic Support System\n",
    "\n",
    "This notebook shows how to run COMPASS on a participant folder and inspect outputs.\n",
    "\n",
    "What you'll do:\n",
    "- Launch the web dashboard (optional)\n",
    "- Run the pipeline in-notebook\n",
    "- Inspect the final report + execution log\n",
    "\n",
    "Prerequisites:\n",
    "- Data under `data/pseudo_data` (demo) or `data/__FEATURES__/COMPASS_data`\n",
    "- If using OpenAI backend, set `OPENAI_API_KEY` in your environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown, JSON, clear_output\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "# --- PATH SETUP ---\n",
    "# We are in /utils. Navigate up two levels to project root (INFERENCE_PIPELINE)\n",
    "project_root = Path.cwd().parent.parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"[*] Project Root: {project_root}\")\n",
    "\n",
    "from multi_agent_system.main import run_compass_pipeline\n",
    "from multi_agent_system.config.settings import get_settings, LLMBackend\n",
    "\n",
    "# Locate Data\n",
    "PSEUDO_DATA_ROOT = project_root / \"multi_agent_system\" / \"data\" / \"pseudo_data\"\n",
    "if not PSEUDO_DATA_ROOT.exists():\n",
    "    # Fallback to main data dir if pseudo_data is missing\n",
    "    PSEUDO_DATA_ROOT = project_root / \"data\" / \"__FEATURES__\" / \"COMPASS_data\"\n",
    "\n",
    "print(f\"\u2705 COMPASS Engine Loaded. Data Source: {PSEUDO_DATA_ROOT.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Launch: Web Dashboard\n",
    "\n",
    "Launch the live UI (`main.py --ui`) for step-by-step monitoring in your browser.\n",
    "This uses the same Python environment as the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_dashboard(b):\n",
    "    clear_output()\n",
    "    print(\"\ud83d\ude80 Launching Dashboard on port 5005...\")\n",
    "    print(\"Go to: http://127.0.0.1:5005\")\n",
    "    try:\n",
    "        # Use sys.executable to ensure we use the same python env\n",
    "        main_script = project_root / \"multi_agent_system\" / \"main.py\"\n",
    "        subprocess.Popen([sys.executable, str(main_script), \"--ui\"])\n",
    "    except Exception as e:\n",
    "        print(f\"Error launching: {e}\")\n",
    "\n",
    "btn = widgets.Button(description=\"Launch Web Dashboard\", button_style='info', icon='rocket')\n",
    "btn.on_click(launch_dashboard)\n",
    "display(btn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Notebook Execution\n",
    "\n",
    "Run the pipeline directly in the notebook (CLI mode).\n",
    "This is useful for quick iteration and debugging without the web dashboard.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Configuration\n",
    "Select a subject, target condition, backend, and number of iterations.\n",
    "If you choose the Local backend, ensure the model is available on this machine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find available subjects\n",
    "subjects = [d.name for d in PSEUDO_DATA_ROOT.iterdir() if d.is_dir() and not d.name.startswith('.')]\n",
    "subjects.sort()\n",
    "\n",
    "# Interactive Widgets\n",
    "subject_dropdown = widgets.Dropdown(\n",
    "    options=subjects,\n",
    "    description='Subject:',\n",
    ")\n",
    "\n",
    "target_input = widgets.Text(\n",
    "    value='Major Depressive Disorder',\n",
    "    description='Target Pattern:',\n",
    "    placeholder='e.g. Anxiety'\n",
    ")\n",
    "\n",
    "iterations_input = widgets.BoundedIntText(\n",
    "    value=2,\n",
    "    min=1,\n",
    "    max=5,\n",
    "    description='Iterations:',\n",
    ")\n",
    "\n",
    "# Backend Selection\n",
    "backend_dropdown = widgets.Dropdown(\n",
    "    options=[('OpenAI API (GPT-5)', 'openai'), ('Local LLM (Open Source)', 'local')],\n",
    "    value='openai',\n",
    "    description='Backend:',\n",
    ")\n",
    "\n",
    "model_input = widgets.Text(\n",
    "    value='Qwen/Qwen2.5-0.5B-Instruct',\n",
    "    description='Local Model:',\n",
    "    disabled=True\n",
    ")\n",
    "\n",
    "def on_backend_change(change):\n",
    "    if change['new'] == 'local':\n",
    "        model_input.disabled = False\n",
    "    else:\n",
    "        model_input.disabled = True\n",
    "\n",
    "backend_dropdown.observe(on_backend_change, names='value')\n",
    "\n",
    "display(subject_dropdown, target_input, iterations_input, backend_dropdown, model_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Run Analysis\n",
    "This executes the pipeline and prints verbose output so you can follow agent reasoning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARTICIPANT_DIR = PSEUDO_DATA_ROOT / subject_dropdown.value\n",
    "TARGET = target_input.value\n",
    "MAX_ITERS = iterations_input.value\n",
    "\n",
    "print(f\"Selected Participant: {PARTICIPANT_DIR}\")\n",
    "\n",
    "# Apply Backend Settings\n",
    "settings = get_settings()\n",
    "if backend_dropdown.value == 'local':\n",
    "    settings.models.backend = LLMBackend.LOCAL\n",
    "    settings.models.local_model_name = model_input.value\n",
    "    print(f\"Configured for LOCAL inference using {model_input.value}\")\n",
    "else:\n",
    "    settings.models.backend = LLMBackend.OPENAI\n",
    "    print(\"Configured for OPENAI inference\")\n",
    "\n",
    "print(f\"Starting analysis for: {TARGET}...\")\n",
    "\n",
    "# Run Pipeline\n",
    "try:\n",
    "    result = run_compass_pipeline(\n",
    "        participant_dir=PARTICIPANT_DIR,\n",
    "        target_condition=TARGET,\n",
    "        max_iterations=MAX_ITERS,\n",
    "        verbose=True,\n",
    "        interactive_ui=False  # Use CLI output mode for notebook compatibility\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Execution Error: {e}\")\n",
    "    result = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Inspector\n",
    "Review the final report and the structured execution log generated by the run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if result:\n",
    "    log_path = Path(result['output_dir']) / f\"execution_log_{result['participant_id']}.json\"\n",
    "    report_path = Path(result['output_dir']) / f\"report_{result['participant_id']}.md\"\n",
    "\n",
    "    print(f\"\\n\ud83d\udcca RESULT: {result['prediction']} ({result['probability']:.1%})\")\n",
    "    \n",
    "    if report_path.exists():\n",
    "        display(Markdown(\"### Final Clinical Report\"))\n",
    "        with open(report_path, 'r') as f:\n",
    "            display(Markdown(f.read()))\n",
    "else:\n",
    "    print(\"No successful result to inspect.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}