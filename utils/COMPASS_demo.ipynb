{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# COMPASS Engine: Interactive Demo\n",
                "\n",
                "### Clinical Orchestrated Multi-modal Predictive Agentic Support System\n",
                "\n",
                "Welcome to the **COMPASS** interactive demo. This notebook demonstrates the system's reasoning capabilities using synthetic clinical data (Pseudodata).\n",
                "\n",
                "**Objective**: To diagnose complex neuropsychiatric conditions by orchestrating multiple AI agents.\n",
                "\n",
                "**Key Features Demonstrated**:\n",
                "- **Dynamic Phenotype Targeting**: You select the diagnosis to hunt for.\n",
                "- **Transparent Reasoning**: View the exact thoughts of the Orchestrator, Tools, and Predictor.\n",
                "- **Multi-Agent Collaboration**: Orchestrator (Plan) -> Executor (Do) -> Integrator (Fuse) -> Predictor (Solve)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import json\n",
                "import ipywidgets as widgets\n",
                "from IPython.display import display, Markdown, JSON\n",
                "from pathlib import Path\n",
                "\n",
                "# Ensure the system path includes the project root (Parent of multi_agent_system)\n",
                "# This allows imports like 'from multi_agent_system.main import ...' to work\n",
                "project_root = Path.cwd().parent.parent\n",
                "if str(project_root) not in sys.path:\n",
                "    sys.path.insert(0, str(project_root))\n",
                "\n",
                "from multi_agent_system.main import run_compass_pipeline\n",
                "from multi_agent_system.config.settings import get_settings, LLMBackend\n",
                "\n",
                "# Settings\n",
                "PSEUDO_DATA_ROOT = Path(\"data/pseudo_data\")\n",
                "\n",
                "print(\"‚úÖ COMPASS Engine Loaded\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Select Participant, Target & Backend\n",
                "\n",
                "Choose a synthetic profile to analyze and select your LLM backend (OpenAI or Local).\n",
                "*   **SUBJ_001**: A profile with features strongly suggestive of Major Depressive Disorder (MDD).\n",
                "*   **SUBJ_002**: A 'Healthy Control' profile with minor incidental findings."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Interactive Widgets\n",
                "subject_dropdown = widgets.Dropdown(\n",
                "    options=[('Subject 001 (MDD Phenotype)', 'SUBJ_001_PSEUDO'), ('Subject 002 (Healthy Control)', 'SUBJ_002_PSEUDO')],\n",
                "    value='SUBJ_001_PSEUDO',\n",
                "    description='Subject:',\n",
                ")\n",
                "\n",
                "target_input = widgets.Text(\n",
                "    value='Major Depressive Disorder',\n",
                "    description='Target:',\n",
                "    placeholder='e.g. Anxiety'\n",
                ")\n",
                "\n",
                "# Backend Selection\n",
                "backend_dropdown = widgets.Dropdown(\n",
                "    options=[('OpenAI API (GPT-5)', 'openai'), ('Local LLM (Open Source)', 'local')],\n",
                "    value='openai',\n",
                "    description='Backend:',\n",
                ")\n",
                "\n",
                "model_input = widgets.Text(\n",
                "    value='Qwen/Qwen2.5-0.5B-Instruct',\n",
                "    description='Local Model:',\n",
                "    disabled=True\n",
                ")\n",
                "\n",
                "def on_backend_change(change):\n",
                "    if change['new'] == 'local':\n",
                "        model_input.disabled = False\n",
                "    else:\n",
                "        model_input.disabled = True\n",
                "\n",
                "backend_dropdown.observe(on_backend_change, names='value')\n",
                "\n",
                "display(subject_dropdown, target_input, backend_dropdown, model_input)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Run Main Pipeline\n",
                "\n",
                "This step launches the **Actor-Critic Loop**.\n",
                "\n",
                "**Watch the Output Below** for real-time logs:\n",
                "1.  **Orchestrator**: \"I need to check the MRI volumes and Lipidomics first...\"\n",
                "2.  **Executor**: Runs `PhenotypeRepresentation` tool.\n",
                "3.  **Integrator (Fusion)**: Merges the tool outputs with the raw clinical notes.\n",
                "4.  **Predictor**: \"Based on the hippocampal atrophy and elevated CRP...\"\n",
                "5.  **Critic**: \"Is this evidence sufficient? Yes.\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "PARTICIPANT_DIR = PSEUDO_DATA_ROOT / subject_dropdown.value\n",
                "TARGET = target_input.value\n",
                "\n",
                "# Apply Backend Settings\n",
                "settings = get_settings()\n",
                "if backend_dropdown.value == 'local':\n",
                "    settings.models.backend = LLMBackend.LOCAL\n",
                "    settings.models.local_model_name = model_input.value\n",
                "    print(f\"‚öôÔ∏è Configured for LOCAL inference using {model_input.value}\")\n",
                "else:\n",
                "    settings.models.backend = LLMBackend.OPENAI\n",
                "    print(\"‚öôÔ∏è Configured for OPENAI inference\")\n",
                "\n",
                "print(f\"üöÄ INITIATING ANALYSIS: {subject_dropdown.value} -> {TARGET}...\")\n",
                "\n",
                "# Run Pipeline (Verbose=True for transparency)\n",
                "result = run_compass_pipeline(\n",
                "    participant_dir=PARTICIPANT_DIR,\n",
                "    target_condition=TARGET,\n",
                "    max_iterations=2,\n",
                "    verbose=True,      # Show agent thoughts\n",
                "    interactive_ui=False # Use CLI output for Notebook clarity\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Transparency Inspector\n",
                "\n",
                "Let's look under the hood at the **Execution Log**. This JSON structure reveals exactly what tools were called and what data was passed."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the structured Execution Log\n",
                "log_path = Path(result['output_dir']) / f\"execution_log_{result['participant_id']}.json\"\n",
                "\n",
                "if log_path.exists():\n",
                "    with open(log_path, 'r') as f:\n",
                "        logs = json.load(f)\n",
                "    \n",
                "    # Display Decision Trace (Simplified)\n",
                "    print(f\"\\nüîç DECISION TRACE for {result['participant_id']}:\")\n",
                "    print(\"-\" * 60)\n",
                "    \n",
                "    # Iterate through logs to find key agent actions\n",
                "    for entry in logs:\n",
                "        if entry['type'] == 'ORCHESTRATOR':\n",
                "            print(f\"üîµ ORCHESTRATOR PLAN: {entry['data']['total_steps']} steps planned.\")\n",
                "        elif entry['type'] == 'PREDICTOR':\n",
                "            print(f\"üü¢ PREDICTOR CONCLUSION: {entry['data']['classification']} ({entry['data']['probability']:.1%})\")\n",
                "        elif entry['type'] == 'CRITIC':\n",
                "            print(f\"üü† CRITIC VERDICT: {entry['data']['verdict']} (Confidence: {entry['data']['confidence']})\")\n",
                "            \n",
                "    print(\"-\" * 60)\n",
                "    # print(\"Full log available at:\", log_path)\n",
                "    print(\"Recent Logs:\")\n",
                "    print(json.dumps(logs[-3:], indent=2)) \n",
                "else:\n",
                "    print(\"Log file not found.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Final Clinical Report\n",
                "\n",
                "The system generates a human-readable report. Here is the final output presented to the clinician."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "report_path = Path(result['output_dir']) / f\"report_{result['participant_id']}.md\"\n",
                "\n",
                "if report_path.exists():\n",
                "    with open(report_path, 'r') as f:\n",
                "        report_content = f.read()\n",
                "    display(Markdown(report_content))\n",
                "else:\n",
                "    print(\"Report not found.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}